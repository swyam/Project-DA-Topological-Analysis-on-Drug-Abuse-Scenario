{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import networkx as nx\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2003\n",
      "30991\n",
      "30991\n",
      "2003\n",
      "<generator object connected_components at 0x7fd9afe59840>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/swyam/.local/lib/python3.7/site-packages/networkx/drawing/nx_pylab.py:611: MatplotlibDeprecationWarning: isinstance(..., numbers.Number)\n",
      "  if cb.is_numlike(alpha):\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<2003x2003 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 61982 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "def load_datax(path=\"da/\"):\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    #print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_ = np.genfromtxt(\"{}{}.txt\".format(path, \"nodelist\"),\n",
    "                                        dtype=int)\n",
    "    print(len(idx_))\n",
    "    #features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    #labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    #idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    #idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    data = pd.read_csv('da/edgelist.txt', sep=\"(\", header=None)\n",
    "    col=data.columns\n",
    "    data=data[col[1]]\n",
    "    data = data.str.split(\",\",expand=True) \n",
    "    col=data.columns\n",
    "    data1 = data[col[0]].str.split(\"{\",expand=True) \n",
    "    data1=data1[data1.columns[1]]\n",
    "    data2 = data[col[1]].str.split(\"}\",expand=True)\n",
    "    col=data2.columns\n",
    "    #print(data1)\n",
    "    data2=data2.drop(columns=col[1], axis = 1) \n",
    "    #print(data2)\n",
    "    #data['new_col'] = list(zip(df.lat, df.long))\n",
    "    data=pd.concat((data1,data2),axis=1)\n",
    "    #data.rename(columns={data.columns[0]:'a',data.columns[1]:'b',data.columns[2]:'c'},inplace=True)\n",
    "    #data = list(zip(data1, data2))\n",
    "    #print(data)\n",
    "    edge=[]\n",
    "    for i in range(len(data.values)):\n",
    "        \n",
    "        edge.append((int(data.values[i][0]),int(data.values[i][1])))\n",
    "    print(len(edge))\n",
    "        \n",
    "    G=nx.Graph()\n",
    "    G.add_nodes_from(idx_)\n",
    "    G.add_edges_from(edge)\n",
    "    print(len(G.edges()))\n",
    "    print(len(G.nodes()))\n",
    "    print(nx.connected_components(G))\n",
    "    options = {\n",
    "    'node_color': 'blue',\n",
    "    'node_size': 0.0005,\n",
    "    'width':0.05,\n",
    "    'arrowstyle': '-|>',\n",
    "    'arrowsize': 1000,\n",
    "    }\n",
    "    nx.draw(G,arrows=True,**options)\n",
    "    plt.savefig(\"graph.png\", dpi=1000)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    #print(data)\n",
    "\n",
    "    adj=nx.adjacency_matrix(G)\n",
    "\n",
    "    return adj\n",
    "load_datax()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def load_data(path=\"../cora/\", dataset=\"cora\"):\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
    "                                        dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}_cites.txt\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize(features)\n",
    "    #adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "    \"\"\"\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "    \n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "    \"\"\"\n",
    "\n",
    "    return adj,features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "        \n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "# Set random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# Settings\n",
    "\n",
    "flags = tf.app.flags\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('epochs', 600, 'Number of epochs to train.')\n",
    "flags.DEFINE_integer('hidden1', 128, 'Number of units in hidden layer 1.')\n",
    "flags.DEFINE_integer('hidden2', 64, 'Number of units in hidden layer 2.')\n",
    "flags.DEFINE_float('dropout', 0.20, 'Dropout rate (1 - keep probability).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def weight_variable_glorot(input_dim, output_dim, name=\"\"):\n",
    "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
    "    initial = tf.random_uniform(\n",
    "        [input_dim, output_dim], minval=-init_range,\n",
    "        maxval=init_range, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def dropout_sparse(x, keep_prob, num_nonzero_elems):\n",
    "    noise_shape = [num_nonzero_elems]\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += tf.random_uniform(noise_shape)\n",
    "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "    pre_out = tf.sparse_retain(x, dropout_mask)\n",
    "    return pre_out * (1. / keep_prob)\n",
    "\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    if not sp.isspmatrix_coo(sparse_mx):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape\n",
    "\n",
    "\n",
    "def preprocess_graph(adj):\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    adj_ = adj + sp.eye(adj.shape[0])\n",
    "    rowsum = np.array(adj_.sum(1))\n",
    "    degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten())\n",
    "    adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo()\n",
    "    return sparse_to_tuple(adj_normalized)\n",
    "\n",
    "\n",
    "def construct_feed_dict(adj_normalized, adj, features, placeholders):\n",
    "    feed_dict = dict()\n",
    "    feed_dict.update({placeholders['features']: features})\n",
    "    feed_dict.update({placeholders['adj']: adj_normalized})\n",
    "    feed_dict.update({placeholders['adj_orig']: adj})\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "def mask_test_edges(adj):\n",
    "    # Function to build test set with 2% positive links\n",
    "    # Remove diagonal elements\n",
    "    adj = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n",
    "    adj.eliminate_zeros()\n",
    "\n",
    "    adj_triu = sp.triu(adj)\n",
    "    adj_tuple = sparse_to_tuple(adj_triu)\n",
    "    edges = adj_tuple[0]\n",
    "    edges_all = sparse_to_tuple(adj)[0]\n",
    "    num_test = int(np.floor(edges.shape[0] / 50.))\n",
    "    num_val = int(np.floor(edges.shape[0] / 50.))\n",
    "\n",
    "    all_edge_idx = list(range(edges.shape[0]))\n",
    "    np.random.shuffle(all_edge_idx)\n",
    "    val_edge_idx = all_edge_idx[:num_val]\n",
    "    test_edge_idx = all_edge_idx[num_val:(num_val + num_test)]\n",
    "    test_edges = edges[test_edge_idx]\n",
    "    val_edges = edges[val_edge_idx]\n",
    "    train_edges = np.delete(edges, np.hstack([test_edge_idx, val_edge_idx]), axis=0)\n",
    "\n",
    "    def ismember(a, b):\n",
    "        rows_close = np.all((a - b[:, None]) == 0, axis=-1)\n",
    "        return np.any(rows_close)\n",
    "\n",
    "    test_edges_false = []\n",
    "    while len(test_edges_false) < len(test_edges):\n",
    "        n_rnd = len(test_edges) - len(test_edges_false)\n",
    "        rnd = np.random.randint(0, adj.shape[0], size=2 * n_rnd)\n",
    "        idxs_i = rnd[:n_rnd]                                        \n",
    "        idxs_j = rnd[n_rnd:]\n",
    "        for i in range(n_rnd):\n",
    "            idx_i = idxs_i[i]\n",
    "            idx_j = idxs_j[i]\n",
    "            if idx_i == idx_j:\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], edges_all):\n",
    "                continue\n",
    "            if test_edges_false:\n",
    "                if ismember([idx_j, idx_i], np.array(test_edges_false)):\n",
    "                    continue\n",
    "                if ismember([idx_i, idx_j], np.array(test_edges_false)):\n",
    "                    continue\n",
    "            test_edges_false.append([idx_i, idx_j])\n",
    "\n",
    "    val_edges_false = []\n",
    "    while len(val_edges_false) < len(val_edges):\n",
    "        n_rnd = len(val_edges) - len(val_edges_false)\n",
    "        rnd = np.random.randint(0, adj.shape[0], size=2 * n_rnd)\n",
    "        idxs_i = rnd[:n_rnd]                                        \n",
    "        idxs_j = rnd[n_rnd:]\n",
    "        for i in range(n_rnd):\n",
    "            idx_i = idxs_i[i]\n",
    "            idx_j = idxs_j[i]\n",
    "            if idx_i == idx_j:\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], train_edges):\n",
    "                continue\n",
    "            if ismember([idx_j, idx_i], train_edges):\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], val_edges):\n",
    "                continue\n",
    "            if ismember([idx_j, idx_i], val_edges):\n",
    "                continue\n",
    "            if val_edges_false:\n",
    "                if ismember([idx_j, idx_i], np.array(val_edges_false)):\n",
    "                    continue\n",
    "                if ismember([idx_i, idx_j], np.array(val_edges_false)):\n",
    "                    continue\n",
    "            val_edges_false.append([idx_i, idx_j])\n",
    "\n",
    "    # Re-build adj matrix\n",
    "    data = np.ones(train_edges.shape[0])\n",
    "    adj_train = sp.csr_matrix((data, (train_edges[:, 0], train_edges[:, 1])), shape=adj.shape)\n",
    "    adj_train = adj_train + adj_train.T\n",
    "\n",
    "    return adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false\n",
    "\n",
    "\n",
    "def get_roc_score(edges_pos, edges_neg):\n",
    "    feed_dict.update({placeholders['dropout']: 0})\n",
    "    emb = sess.run(model.embeddings, feed_dict=feed_dict)\n",
    "\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # Predict on test set of edges\n",
    "    adj_rec = np.dot(emb, emb.T)\n",
    "    preds = []\n",
    "    pos = []\n",
    "    for e in edges_pos:\n",
    "        preds.append(sigmoid(adj_rec[e[0], e[1]]))\n",
    "        pos.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_neg = []\n",
    "    neg = []\n",
    "    for e in edges_neg:\n",
    "        preds_neg.append(sigmoid(adj_rec[e[0], e[1]]))\n",
    "        neg.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_all = np.hstack([preds, preds_neg])\n",
    "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds))])\n",
    "    roc_score = roc_auc_score(labels_all, preds_all)\n",
    "    ap_score = average_precision_score(labels_all, preds_all)\n",
    "\n",
    "    return roc_score, ap_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution():\n",
    "    \"\"\"Basic graph convolution layer for undirected graph without edge labels.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, adj, name, dropout=0., act=tf.nn.relu):\n",
    "        self.name = name\n",
    "        self.vars = {}\n",
    "        self.issparse = False\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['weights'] = weight_variable_glorot(input_dim, output_dim, name='weights')\n",
    "        self.dropout = dropout\n",
    "        self.adj = adj\n",
    "        self.act = act\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):        \n",
    "            x = inputs\n",
    "            x = tf.nn.dropout(x, 1-self.dropout)\n",
    "            x = tf.matmul(x, self.vars['weights'])\n",
    "            x = tf.sparse_tensor_dense_matmul(self.adj, x)\n",
    "            outputs = self.act(x)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class GraphConvolutionSparse():\n",
    "    \"\"\"Graph convolution layer for sparse inputs.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, adj, features_nonzero, name, dropout=0., act=tf.nn.relu):\n",
    "        self.name = name\n",
    "        self.vars = {}\n",
    "        self.issparse = False\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['weights'] = weight_variable_glorot(input_dim, output_dim, name='weights')\n",
    "        self.dropout = dropout\n",
    "        self.adj = adj\n",
    "        self.act = act\n",
    "        self.issparse = True\n",
    "        self.features_nonzero = features_nonzero\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):\n",
    "            x = inputs\n",
    "            x = dropout_sparse(x, 1-self.dropout, self.features_nonzero)\n",
    "            x = tf.sparse_tensor_dense_matmul(x, self.vars['weights'])\n",
    "            x = tf.sparse_tensor_dense_matmul(self.adj, x)\n",
    "            outputs = self.act(x)\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "class InnerProductDecoder():\n",
    "    \"\"\"Decoder model layer for link prediction.\"\"\"\n",
    "    def __init__(self, input_dim, name, dropout=0., act=tf.nn.sigmoid):\n",
    "        self.name = name\n",
    "        self.issparse = False\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):\n",
    "            inputs = tf.nn.dropout(inputs, 1-self.dropout)\n",
    "            x = tf.transpose(inputs)\n",
    "            x = tf.matmul(inputs, x)\n",
    "            x = tf.reshape(x, [-1])\n",
    "            outputs = self.act(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNModel():\n",
    "    def __init__(self, placeholders, num_features, features_nonzero, name):\n",
    "        self.name = name\n",
    "        self.inputs = placeholders['features']\n",
    "        self.input_dim = num_features\n",
    "        self.features_nonzero = features_nonzero\n",
    "        self.adj = placeholders['adj']\n",
    "        self.dropout = placeholders['dropout']\n",
    "        with tf.variable_scope(self.name):\n",
    "            self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        self.hidden1 = GraphConvolutionSparse(\n",
    "            name='gcn_sparse_layer',\n",
    "            input_dim=self.input_dim,\n",
    "            output_dim=FLAGS.hidden1,\n",
    "            adj=self.adj,\n",
    "            features_nonzero=self.features_nonzero,\n",
    "            act=tf.nn.relu,\n",
    "            dropout=self.dropout)(self.inputs)\n",
    "\n",
    "        self.embeddings = GraphConvolution(\n",
    "            name='gcn_dense_layer',\n",
    "            input_dim=FLAGS.hidden1,\n",
    "            output_dim=FLAGS.hidden2,\n",
    "            adj=self.adj,\n",
    "            act=lambda x: x,\n",
    "            dropout=self.dropout)(self.hidden1)\n",
    "\n",
    "        self.reconstructions = InnerProductDecoder(\n",
    "            name='gcn_decoder',\n",
    "            input_dim=FLAGS.hidden2, \n",
    "            act=lambda x: x)(self.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    def __init__(self, preds, labels, num_nodes, num_edges):\n",
    "        pos_weight = float(num_nodes**2 - num_edges) / num_edges\n",
    "        norm = num_nodes**2 / float((num_nodes**2 - num_edges) * 2)\n",
    "        \n",
    "        preds_sub = preds\n",
    "        labels_sub = labels\n",
    "\n",
    "        self.cost = norm * tf.reduce_mean(\n",
    "            tf.nn.weighted_cross_entropy_with_logits(\n",
    "                logits=preds_sub, targets=labels_sub, pos_weight=pos_weight))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)  # Adam Optimizer\n",
    "\n",
    "        self.opt_op = self.optimizer.minimize(self.cost)\n",
    "        self.grads_vars = self.optimizer.compute_gradients(self.cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2003\n",
      "30991\n",
      "30991\n",
      "2003\n",
      "<generator object connected_components at 0x7fd9aec4c930>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "adj = load_datax()\n",
    "#adj,features=load_data()\n",
    "num_nodes = adj.shape[0]\n",
    "num_edges = adj.sum()\n",
    "# Featureless\n",
    "features = sparse_to_tuple(sp.identity(num_nodes))\n",
    "#features = sparse_to_tuple(features)\n",
    "num_features = features[2][1]\n",
    "features_nonzero = features[1].shape[0]\n",
    "\n",
    "# Store original adjacency matrix (without diagonal entries) for later\n",
    "adj_orig = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n",
    "adj_orig.eliminate_zeros()\n",
    "\n",
    "adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj)\n",
    "adj = adj_train\n",
    "\n",
    "adj_norm = preprocess_graph(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodes: 2003\n",
      "Training edges (positive): 29753\n",
      "Validation edges (positive): 619\n",
      "Validation edges (negative): 619\n",
      "Test edges (positive): 619\n",
      "Test edges (negative): 619\n"
     ]
    }
   ],
   "source": [
    "# Inspect train/test split\n",
    "print (\"Total nodes:\", adj.shape[0])\n",
    "#print \"Total edges:\", int(adj_sparse.nnz/2) # adj is symmetric, so nnz (num non-zero) = 2*num_edges\n",
    "print (\"Training edges (positive):\", len(train_edges))\n",
    "#print \"Training edges (negative):\", len(train_edges_false)\n",
    "print (\"Validation edges (positive):\", len(val_edges))\n",
    "print (\"Validation edges (negative):\", len(val_edges_false))\n",
    "print (\"Test edges (positive):\", len(test_edges))\n",
    "print (\"Test edges (negative):\", len(test_edges_false))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-6-e06c5c3ce1fe>:16: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Define placeholders\n",
    "placeholders = {\n",
    "    'features': tf.sparse_placeholder(tf.float32),\n",
    "    'adj': tf.sparse_placeholder(tf.float32),\n",
    "    'adj_orig': tf.sparse_placeholder(tf.float32),\n",
    "    'dropout': tf.placeholder_with_default(0., shape=())\n",
    "}\n",
    "\n",
    "# Create model\n",
    "model = GCNModel(placeholders, num_features, features_nonzero, name='cora')\n",
    "\n",
    "# Create optimizer\n",
    "with tf.name_scope('optimizer'):\n",
    "    opt = Optimizer(\n",
    "        preds=model.reconstructions,\n",
    "        labels=tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'], validate_indices=False), [-1]),\n",
    "        num_nodes=num_nodes,\n",
    "        num_edges=num_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 0.69038 val_roc= 0.92988 val_ap= 0.93587 time= 1.08183\n",
      "Epoch: 0002 train_loss= 0.68604 val_roc= 0.94893 val_ap= 0.95108 time= 0.42902\n",
      "Epoch: 0003 train_loss= 0.66019 val_roc= 0.95037 val_ap= 0.95256 time= 0.38579\n",
      "Epoch: 0004 train_loss= 0.60139 val_roc= 0.94970 val_ap= 0.95263 time= 0.38615\n",
      "Epoch: 0005 train_loss= 0.54960 val_roc= 0.94976 val_ap= 0.95300 time= 0.39488\n",
      "Epoch: 0006 train_loss= 0.59240 val_roc= 0.94916 val_ap= 0.95281 time= 0.41718\n",
      "Epoch: 0007 train_loss= 0.58195 val_roc= 0.94815 val_ap= 0.95234 time= 0.41492\n",
      "Epoch: 0008 train_loss= 0.54538 val_roc= 0.94735 val_ap= 0.95179 time= 0.39210\n",
      "Epoch: 0009 train_loss= 0.53552 val_roc= 0.94610 val_ap= 0.95086 time= 0.38648\n",
      "Epoch: 0010 train_loss= 0.54347 val_roc= 0.94465 val_ap= 0.94981 time= 0.38426\n",
      "Epoch: 0011 train_loss= 0.55071 val_roc= 0.94337 val_ap= 0.94880 time= 0.41306\n",
      "Epoch: 0012 train_loss= 0.55019 val_roc= 0.94249 val_ap= 0.94806 time= 0.42550\n",
      "Epoch: 0013 train_loss= 0.53731 val_roc= 0.94180 val_ap= 0.94754 time= 0.38715\n",
      "Epoch: 0014 train_loss= 0.52538 val_roc= 0.94113 val_ap= 0.94703 time= 0.40221\n",
      "Epoch: 0015 train_loss= 0.52208 val_roc= 0.94061 val_ap= 0.94663 time= 0.39136\n",
      "Epoch: 0016 train_loss= 0.52671 val_roc= 0.94004 val_ap= 0.94617 time= 0.40873\n",
      "Epoch: 0017 train_loss= 0.53083 val_roc= 0.93915 val_ap= 0.94545 time= 0.43150\n",
      "Epoch: 0018 train_loss= 0.52991 val_roc= 0.93802 val_ap= 0.94458 time= 0.39667\n",
      "Epoch: 0019 train_loss= 0.51777 val_roc= 0.93672 val_ap= 0.94360 time= 0.39025\n",
      "Epoch: 0020 train_loss= 0.51480 val_roc= 0.93502 val_ap= 0.94240 time= 0.39636\n",
      "Epoch: 0021 train_loss= 0.51445 val_roc= 0.93263 val_ap= 0.94095 time= 0.40171\n",
      "Epoch: 0022 train_loss= 0.51495 val_roc= 0.93102 val_ap= 0.93999 time= 0.43194\n",
      "Epoch: 0023 train_loss= 0.51361 val_roc= 0.92906 val_ap= 0.93884 time= 0.38603\n",
      "Epoch: 0024 train_loss= 0.51450 val_roc= 0.92735 val_ap= 0.93800 time= 0.39072\n",
      "Epoch: 0025 train_loss= 0.51036 val_roc= 0.92644 val_ap= 0.93749 time= 0.38546\n",
      "Epoch: 0026 train_loss= 0.50822 val_roc= 0.92543 val_ap= 0.93692 time= 0.40769\n",
      "Epoch: 0027 train_loss= 0.50528 val_roc= 0.92381 val_ap= 0.93611 time= 0.42782\n",
      "Epoch: 0028 train_loss= 0.50478 val_roc= 0.92166 val_ap= 0.93496 time= 0.40733\n",
      "Epoch: 0029 train_loss= 0.50370 val_roc= 0.91891 val_ap= 0.93347 time= 0.39068\n",
      "Epoch: 0030 train_loss= 0.50181 val_roc= 0.91522 val_ap= 0.93143 time= 0.38750\n",
      "Epoch: 0031 train_loss= 0.50173 val_roc= 0.91185 val_ap= 0.92935 time= 0.39379\n",
      "Epoch: 0032 train_loss= 0.50103 val_roc= 0.90761 val_ap= 0.92652 time= 0.41993\n",
      "Epoch: 0033 train_loss= 0.49867 val_roc= 0.90406 val_ap= 0.92369 time= 0.42087\n",
      "Epoch: 0034 train_loss= 0.49826 val_roc= 0.90111 val_ap= 0.92102 time= 0.38827\n",
      "Epoch: 0035 train_loss= 0.49831 val_roc= 0.89946 val_ap= 0.91937 time= 0.39005\n",
      "Epoch: 0036 train_loss= 0.49877 val_roc= 0.89899 val_ap= 0.91898 time= 0.39432\n",
      "Epoch: 0037 train_loss= 0.49710 val_roc= 0.89949 val_ap= 0.91954 time= 0.43514\n",
      "Epoch: 0038 train_loss= 0.49590 val_roc= 0.90014 val_ap= 0.92018 time= 0.39990\n",
      "Epoch: 0039 train_loss= 0.49613 val_roc= 0.90143 val_ap= 0.92129 time= 0.39181\n",
      "Epoch: 0040 train_loss= 0.49665 val_roc= 0.90227 val_ap= 0.92206 time= 0.38919\n",
      "Epoch: 0041 train_loss= 0.49679 val_roc= 0.90237 val_ap= 0.92220 time= 0.39954\n",
      "Epoch: 0042 train_loss= 0.49611 val_roc= 0.90236 val_ap= 0.92212 time= 0.42419\n",
      "Epoch: 0043 train_loss= 0.49502 val_roc= 0.90205 val_ap= 0.92180 time= 0.44302\n",
      "Epoch: 0044 train_loss= 0.49416 val_roc= 0.90144 val_ap= 0.92121 time= 0.38469\n",
      "Epoch: 0045 train_loss= 0.49345 val_roc= 0.90144 val_ap= 0.92117 time= 0.38387\n",
      "Epoch: 0046 train_loss= 0.49523 val_roc= 0.90336 val_ap= 0.92255 time= 0.39016\n",
      "Epoch: 0047 train_loss= 0.49471 val_roc= 0.90611 val_ap= 0.92459 time= 0.43604\n",
      "Epoch: 0048 train_loss= 0.49227 val_roc= 0.90838 val_ap= 0.92636 time= 0.43138\n",
      "Epoch: 0049 train_loss= 0.49189 val_roc= 0.91015 val_ap= 0.92781 time= 0.39214\n",
      "Epoch: 0050 train_loss= 0.49178 val_roc= 0.91150 val_ap= 0.92886 time= 0.39112\n",
      "Epoch: 0051 train_loss= 0.49139 val_roc= 0.91265 val_ap= 0.92974 time= 0.39080\n",
      "Epoch: 0052 train_loss= 0.49504 val_roc= 0.91274 val_ap= 0.92984 time= 0.42814\n",
      "Epoch: 0053 train_loss= 0.49432 val_roc= 0.91158 val_ap= 0.92909 time= 0.43869\n",
      "Epoch: 0054 train_loss= 0.49266 val_roc= 0.90962 val_ap= 0.92782 time= 0.40821\n",
      "Epoch: 0055 train_loss= 0.49109 val_roc= 0.90778 val_ap= 0.92664 time= 0.42369\n",
      "Epoch: 0056 train_loss= 0.49800 val_roc= 0.90820 val_ap= 0.92705 time= 0.39223\n",
      "Epoch: 0057 train_loss= 0.49120 val_roc= 0.90901 val_ap= 0.92775 time= 0.46443\n",
      "Epoch: 0058 train_loss= 0.49064 val_roc= 0.91017 val_ap= 0.92867 time= 0.42169\n",
      "Epoch: 0059 train_loss= 0.49036 val_roc= 0.91150 val_ap= 0.92967 time= 0.41549\n",
      "Epoch: 0060 train_loss= 0.49117 val_roc= 0.91182 val_ap= 0.93001 time= 0.40699\n",
      "Epoch: 0061 train_loss= 0.48988 val_roc= 0.91144 val_ap= 0.92986 time= 0.38993\n",
      "Epoch: 0062 train_loss= 0.48987 val_roc= 0.91064 val_ap= 0.92942 time= 0.43625\n",
      "Epoch: 0063 train_loss= 0.48985 val_roc= 0.90954 val_ap= 0.92879 time= 0.44224\n",
      "Epoch: 0064 train_loss= 0.48958 val_roc= 0.90896 val_ap= 0.92843 time= 0.38719\n",
      "Epoch: 0065 train_loss= 0.48957 val_roc= 0.90792 val_ap= 0.92774 time= 0.38022\n",
      "Epoch: 0066 train_loss= 0.49282 val_roc= 0.90845 val_ap= 0.92819 time= 0.37866\n",
      "Epoch: 0067 train_loss= 0.49352 val_roc= 0.91034 val_ap= 0.92954 time= 0.43580\n",
      "Epoch: 0068 train_loss= 0.49018 val_roc= 0.91144 val_ap= 0.93036 time= 0.42922\n",
      "Epoch: 0069 train_loss= 0.49419 val_roc= 0.91077 val_ap= 0.92993 time= 0.40437\n",
      "Epoch: 0070 train_loss= 0.49410 val_roc= 0.90850 val_ap= 0.92837 time= 0.38592\n",
      "Epoch: 0071 train_loss= 0.48855 val_roc= 0.90587 val_ap= 0.92658 time= 0.39377\n",
      "Epoch: 0072 train_loss= 0.48947 val_roc= 0.90412 val_ap= 0.92526 time= 0.42257\n",
      "Epoch: 0073 train_loss= 0.49049 val_roc= 0.90400 val_ap= 0.92517 time= 0.41902\n",
      "Epoch: 0074 train_loss= 0.49232 val_roc= 0.90582 val_ap= 0.92655 time= 0.38843\n",
      "Epoch: 0075 train_loss= 0.48862 val_roc= 0.90809 val_ap= 0.92815 time= 0.38351\n",
      "Epoch: 0076 train_loss= 0.48851 val_roc= 0.90965 val_ap= 0.92926 time= 0.39401\n",
      "Epoch: 0077 train_loss= 0.48874 val_roc= 0.91100 val_ap= 0.93017 time= 0.42952\n",
      "Epoch: 0078 train_loss= 0.48831 val_roc= 0.91184 val_ap= 0.93077 time= 0.41381\n",
      "Epoch: 0079 train_loss= 0.48990 val_roc= 0.91149 val_ap= 0.93053 time= 0.40378\n",
      "Epoch: 0080 train_loss= 0.48808 val_roc= 0.91058 val_ap= 0.92992 time= 0.39357\n",
      "Epoch: 0081 train_loss= 0.48770 val_roc= 0.90965 val_ap= 0.92932 time= 0.38319\n",
      "Epoch: 0082 train_loss= 0.48759 val_roc= 0.90848 val_ap= 0.92859 time= 0.42104\n",
      "Epoch: 0083 train_loss= 0.48778 val_roc= 0.90794 val_ap= 0.92823 time= 0.42712\n",
      "Epoch: 0084 train_loss= 0.48962 val_roc= 0.90901 val_ap= 0.92895 time= 0.39332\n",
      "Epoch: 0085 train_loss= 0.48844 val_roc= 0.91096 val_ap= 0.93020 time= 0.40595\n",
      "Epoch: 0086 train_loss= 0.48798 val_roc= 0.91236 val_ap= 0.93111 time= 0.39446\n",
      "Epoch: 0087 train_loss= 0.48803 val_roc= 0.91280 val_ap= 0.93145 time= 0.41448\n",
      "Epoch: 0088 train_loss= 0.48766 val_roc= 0.91252 val_ap= 0.93127 time= 0.43192\n",
      "Epoch: 0089 train_loss= 0.48986 val_roc= 0.91080 val_ap= 0.93017 time= 0.40950\n",
      "Epoch: 0090 train_loss= 0.48776 val_roc= 0.90890 val_ap= 0.92890 time= 0.39857\n",
      "Epoch: 0091 train_loss= 0.48790 val_roc= 0.90807 val_ap= 0.92841 time= 0.38781\n",
      "Epoch: 0092 train_loss= 0.48917 val_roc= 0.90916 val_ap= 0.92917 time= 0.41908\n",
      "Epoch: 0093 train_loss= 0.48760 val_roc= 0.91090 val_ap= 0.93034 time= 0.42135\n",
      "Epoch: 0094 train_loss= 0.48688 val_roc= 0.91252 val_ap= 0.93138 time= 0.39066\n",
      "Epoch: 0095 train_loss= 0.48787 val_roc= 0.91321 val_ap= 0.93184 time= 0.39278\n",
      "Epoch: 0096 train_loss= 0.48658 val_roc= 0.91374 val_ap= 0.93218 time= 0.40203\n",
      "Epoch: 0097 train_loss= 0.48722 val_roc= 0.91369 val_ap= 0.93217 time= 0.43294\n",
      "Epoch: 0098 train_loss= 0.48654 val_roc= 0.91362 val_ap= 0.93214 time= 0.42434\n",
      "Epoch: 0099 train_loss= 0.48918 val_roc= 0.91209 val_ap= 0.93119 time= 0.38216\n",
      "Epoch: 0100 train_loss= 0.48979 val_roc= 0.90878 val_ap= 0.92899 time= 0.38774\n",
      "Epoch: 0101 train_loss= 0.48692 val_roc= 0.90533 val_ap= 0.92656 time= 0.39856\n",
      "Epoch: 0102 train_loss= 0.49803 val_roc= 0.90629 val_ap= 0.92718 time= 0.39385\n",
      "Epoch: 0103 train_loss= 0.49103 val_roc= 0.90961 val_ap= 0.92947 time= 0.42267\n",
      "Epoch: 0104 train_loss= 0.48691 val_roc= 0.91153 val_ap= 0.93077 time= 0.39774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0105 train_loss= 0.48647 val_roc= 0.91290 val_ap= 0.93167 time= 0.39872\n",
      "Epoch: 0106 train_loss= 0.48704 val_roc= 0.91354 val_ap= 0.93211 time= 0.38667\n",
      "Epoch: 0107 train_loss= 0.48651 val_roc= 0.91414 val_ap= 0.93247 time= 0.39557\n",
      "Epoch: 0108 train_loss= 0.48679 val_roc= 0.91402 val_ap= 0.93243 time= 0.43165\n",
      "Epoch: 0109 train_loss= 0.48757 val_roc= 0.91279 val_ap= 0.93165 time= 0.38819\n",
      "Epoch: 0110 train_loss= 0.48682 val_roc= 0.91265 val_ap= 0.93156 time= 0.39605\n",
      "Epoch: 0111 train_loss= 0.48752 val_roc= 0.91356 val_ap= 0.93219 time= 0.38944\n",
      "Epoch: 0112 train_loss= 0.48609 val_roc= 0.91443 val_ap= 0.93277 time= 0.38954\n",
      "Epoch: 0113 train_loss= 0.48952 val_roc= 0.91351 val_ap= 0.93220 time= 0.43281\n",
      "Epoch: 0114 train_loss= 0.48710 val_roc= 0.91176 val_ap= 0.93106 time= 0.40003\n",
      "Epoch: 0115 train_loss= 0.48629 val_roc= 0.91011 val_ap= 0.92997 time= 0.38759\n",
      "Epoch: 0116 train_loss= 0.48683 val_roc= 0.90941 val_ap= 0.92955 time= 0.39727\n",
      "Epoch: 0117 train_loss= 0.49062 val_roc= 0.91128 val_ap= 0.93086 time= 0.39387\n",
      "Epoch: 0118 train_loss= 0.48670 val_roc= 0.91355 val_ap= 0.93233 time= 0.43110\n",
      "Epoch: 0119 train_loss= 0.48596 val_roc= 0.91519 val_ap= 0.93339 time= 0.41486\n",
      "Epoch: 0120 train_loss= 0.48781 val_roc= 0.91529 val_ap= 0.93348 time= 0.39536\n",
      "Epoch: 0121 train_loss= 0.48850 val_roc= 0.91389 val_ap= 0.93259 time= 0.45087\n",
      "Epoch: 0122 train_loss= 0.48635 val_roc= 0.91196 val_ap= 0.93138 time= 0.39374\n",
      "Epoch: 0123 train_loss= 0.48603 val_roc= 0.91011 val_ap= 0.93014 time= 0.42911\n",
      "Epoch: 0124 train_loss= 0.48599 val_roc= 0.90909 val_ap= 0.92946 time= 0.40296\n",
      "Epoch: 0125 train_loss= 0.48713 val_roc= 0.90953 val_ap= 0.92975 time= 0.41493\n",
      "Epoch: 0126 train_loss= 0.48974 val_roc= 0.91231 val_ap= 0.93161 time= 0.42085\n",
      "Epoch: 0127 train_loss= 0.48991 val_roc= 0.91614 val_ap= 0.93407 time= 0.42197\n",
      "Epoch: 0128 train_loss= 0.48641 val_roc= 0.91828 val_ap= 0.93538 time= 0.52575\n",
      "Epoch: 0129 train_loss= 0.48748 val_roc= 0.91915 val_ap= 0.93583 time= 0.44121\n",
      "Epoch: 0130 train_loss= 0.49036 val_roc= 0.91799 val_ap= 0.93509 time= 0.42114\n",
      "Epoch: 0131 train_loss= 0.49121 val_roc= 0.91450 val_ap= 0.93295 time= 0.37902\n",
      "Epoch: 0132 train_loss= 0.48541 val_roc= 0.91091 val_ap= 0.93064 time= 0.39871\n",
      "Epoch: 0133 train_loss= 0.48652 val_roc= 0.90872 val_ap= 0.92920 time= 0.41773\n",
      "Epoch: 0134 train_loss= 0.48912 val_roc= 0.90914 val_ap= 0.92944 time= 0.39327\n",
      "Epoch: 0135 train_loss= 0.49118 val_roc= 0.91237 val_ap= 0.93153 time= 0.38380\n",
      "Epoch: 0136 train_loss= 0.48588 val_roc= 0.91522 val_ap= 0.93324 time= 0.37413\n",
      "Epoch: 0137 train_loss= 0.48585 val_roc= 0.91788 val_ap= 0.93482 time= 0.40201\n",
      "Epoch: 0138 train_loss= 0.48753 val_roc= 0.91853 val_ap= 0.93522 time= 0.43671\n",
      "Epoch: 0139 train_loss= 0.48908 val_roc= 0.91736 val_ap= 0.93452 time= 0.39908\n",
      "Epoch: 0140 train_loss= 0.48863 val_roc= 0.91413 val_ap= 0.93268 time= 0.37644\n",
      "Epoch: 0141 train_loss= 0.48623 val_roc= 0.90998 val_ap= 0.93002 time= 0.37975\n",
      "Epoch: 0142 train_loss= 0.48687 val_roc= 0.90749 val_ap= 0.92840 time= 0.38597\n",
      "Epoch: 0143 train_loss= 0.48952 val_roc= 0.90775 val_ap= 0.92862 time= 0.41386\n",
      "Epoch: 0144 train_loss= 0.48823 val_roc= 0.91024 val_ap= 0.93031 time= 0.41051\n",
      "Epoch: 0145 train_loss= 0.48629 val_roc= 0.91304 val_ap= 0.93224 time= 0.37122\n",
      "Epoch: 0146 train_loss= 0.48554 val_roc= 0.91515 val_ap= 0.93347 time= 0.37409\n",
      "Epoch: 0147 train_loss= 0.48524 val_roc= 0.91677 val_ap= 0.93446 time= 0.37328\n",
      "Epoch: 0148 train_loss= 0.48553 val_roc= 0.91797 val_ap= 0.93514 time= 0.40745\n",
      "Epoch: 0149 train_loss= 0.48848 val_roc= 0.91716 val_ap= 0.93467 time= 0.41495\n",
      "Epoch: 0150 train_loss= 0.48622 val_roc= 0.91523 val_ap= 0.93352 time= 0.38155\n",
      "Epoch: 0151 train_loss= 0.48646 val_roc= 0.91219 val_ap= 0.93166 time= 0.37346\n",
      "Epoch: 0152 train_loss= 0.48523 val_roc= 0.90849 val_ap= 0.92924 time= 0.42399\n",
      "Epoch: 0153 train_loss= 0.48828 val_roc= 0.90711 val_ap= 0.92830 time= 0.58814\n",
      "Epoch: 0154 train_loss= 0.48708 val_roc= 0.90832 val_ap= 0.92908 time= 0.44832\n",
      "Epoch: 0155 train_loss= 0.48535 val_roc= 0.91022 val_ap= 0.93029 time= 0.37785\n",
      "Epoch: 0156 train_loss= 0.48635 val_roc= 0.91315 val_ap= 0.93212 time= 0.37613\n",
      "Epoch: 0157 train_loss= 0.48751 val_roc= 0.91745 val_ap= 0.93463 time= 0.38192\n",
      "Epoch: 0158 train_loss= 0.48512 val_roc= 0.92040 val_ap= 0.93639 time= 0.40945\n",
      "Epoch: 0159 train_loss= 0.50463 val_roc= 0.91857 val_ap= 0.93530 time= 0.41254\n",
      "Epoch: 0160 train_loss= 0.48758 val_roc= 0.91521 val_ap= 0.93316 time= 0.37568\n",
      "Epoch: 0161 train_loss= 0.48504 val_roc= 0.91092 val_ap= 0.93048 time= 0.37173\n",
      "Epoch: 0162 train_loss= 0.48867 val_roc= 0.90909 val_ap= 0.92930 time= 0.37939\n",
      "Epoch: 0163 train_loss= 0.48645 val_roc= 0.90905 val_ap= 0.92928 time= 0.39558\n",
      "Epoch: 0164 train_loss= 0.48815 val_roc= 0.91147 val_ap= 0.93089 time= 0.41847\n",
      "Epoch: 0165 train_loss= 0.48524 val_roc= 0.91436 val_ap= 0.93270 time= 0.38003\n",
      "Epoch: 0166 train_loss= 0.48451 val_roc= 0.91737 val_ap= 0.93451 time= 0.37794\n",
      "Epoch: 0167 train_loss= 0.48548 val_roc= 0.91877 val_ap= 0.93542 time= 0.37175\n",
      "Epoch: 0168 train_loss= 0.48515 val_roc= 0.91905 val_ap= 0.93559 time= 0.38120\n",
      "Epoch: 0169 train_loss= 0.48753 val_roc= 0.91765 val_ap= 0.93473 time= 0.41688\n",
      "Epoch: 0170 train_loss= 0.48728 val_roc= 0.91431 val_ap= 0.93277 time= 0.39315\n",
      "Epoch: 0171 train_loss= 0.48438 val_roc= 0.91075 val_ap= 0.93057 time= 0.37754\n",
      "Epoch: 0172 train_loss= 0.48662 val_roc= 0.90887 val_ap= 0.92934 time= 0.37989\n",
      "Epoch: 0173 train_loss= 0.48478 val_roc= 0.90832 val_ap= 0.92898 time= 0.38024\n",
      "Epoch: 0174 train_loss= 0.48553 val_roc= 0.90939 val_ap= 0.92972 time= 0.42395\n",
      "Epoch: 0175 train_loss= 0.48717 val_roc= 0.91267 val_ap= 0.93185 time= 0.40541\n",
      "Epoch: 0176 train_loss= 0.48509 val_roc= 0.91640 val_ap= 0.93410 time= 0.37039\n",
      "Epoch: 0177 train_loss= 0.48886 val_roc= 0.91695 val_ap= 0.93447 time= 0.37372\n",
      "Epoch: 0178 train_loss= 0.48402 val_roc= 0.91724 val_ap= 0.93466 time= 0.37728\n",
      "Epoch: 0179 train_loss= 0.48435 val_roc= 0.91719 val_ap= 0.93466 time= 0.40503\n",
      "Epoch: 0180 train_loss= 0.48793 val_roc= 0.91493 val_ap= 0.93336 time= 0.40565\n",
      "Epoch: 0181 train_loss= 0.48392 val_roc= 0.91349 val_ap= 0.93256 time= 0.36835\n",
      "Epoch: 0182 train_loss= 0.48361 val_roc= 0.91275 val_ap= 0.93216 time= 0.38185\n",
      "Epoch: 0183 train_loss= 0.48344 val_roc= 0.91271 val_ap= 0.93214 time= 0.37987\n",
      "Epoch: 0184 train_loss= 0.48308 val_roc= 0.91302 val_ap= 0.93236 time= 0.39146\n",
      "Epoch: 0185 train_loss= 0.48325 val_roc= 0.91376 val_ap= 0.93279 time= 0.42000\n",
      "Epoch: 0186 train_loss= 0.49056 val_roc= 0.91725 val_ap= 0.93481 time= 0.38100\n",
      "Epoch: 0187 train_loss= 0.48307 val_roc= 0.92020 val_ap= 0.93648 time= 0.37172\n",
      "Epoch: 0188 train_loss= 0.49564 val_roc= 0.91900 val_ap= 0.93577 time= 0.37223\n",
      "Epoch: 0189 train_loss= 0.48785 val_roc= 0.91537 val_ap= 0.93376 time= 0.37945\n",
      "Epoch: 0190 train_loss= 0.48500 val_roc= 0.90999 val_ap= 0.93048 time= 0.40748\n",
      "Epoch: 0191 train_loss= 0.48346 val_roc= 0.90613 val_ap= 0.92788 time= 0.40073\n",
      "Epoch: 0192 train_loss= 0.49767 val_roc= 0.90785 val_ap= 0.92901 time= 0.39176\n",
      "Epoch: 0193 train_loss= 0.48803 val_roc= 0.91238 val_ap= 0.93196 time= 0.37650\n",
      "Epoch: 0194 train_loss= 0.48313 val_roc= 0.91759 val_ap= 0.93495 time= 0.37803\n",
      "Epoch: 0195 train_loss= 0.48359 val_roc= 0.92049 val_ap= 0.93666 time= 0.54176\n",
      "Epoch: 0196 train_loss= 0.49366 val_roc= 0.91972 val_ap= 0.93617 time= 0.39761\n",
      "Epoch: 0197 train_loss= 0.48443 val_roc= 0.91706 val_ap= 0.93461 time= 0.50014\n",
      "Epoch: 0198 train_loss= 0.48211 val_roc= 0.91362 val_ap= 0.93264 time= 0.37868\n",
      "Epoch: 0199 train_loss= 0.48704 val_roc= 0.91288 val_ap= 0.93218 time= 0.38117\n",
      "Epoch: 0200 train_loss= 0.48356 val_roc= 0.91372 val_ap= 0.93265 time= 0.41606\n",
      "Epoch: 0201 train_loss= 0.48258 val_roc= 0.91581 val_ap= 0.93392 time= 0.39578\n",
      "Epoch: 0202 train_loss= 0.48186 val_roc= 0.91836 val_ap= 0.93543 time= 0.38039\n",
      "Epoch: 0203 train_loss= 0.48239 val_roc= 0.92150 val_ap= 0.93717 time= 0.37203\n",
      "Epoch: 0204 train_loss= 0.48348 val_roc= 0.92251 val_ap= 0.93778 time= 0.37074\n",
      "Epoch: 0205 train_loss= 0.48410 val_roc= 0.92187 val_ap= 0.93736 time= 0.41438\n",
      "Epoch: 0206 train_loss= 0.48993 val_roc= 0.91812 val_ap= 0.93508 time= 0.40499\n",
      "Epoch: 0207 train_loss= 0.48236 val_roc= 0.91504 val_ap= 0.93319 time= 0.38362\n",
      "Epoch: 0208 train_loss= 0.48286 val_roc= 0.91369 val_ap= 0.93227 time= 0.37347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0209 train_loss= 0.48115 val_roc= 0.91323 val_ap= 0.93192 time= 0.37465\n",
      "Epoch: 0210 train_loss= 0.48139 val_roc= 0.91285 val_ap= 0.93164 time= 0.39998\n",
      "Epoch: 0211 train_loss= 0.48309 val_roc= 0.91446 val_ap= 0.93268 time= 0.41563\n",
      "Epoch: 0212 train_loss= 0.48262 val_roc= 0.91723 val_ap= 0.93448 time= 0.37695\n",
      "Epoch: 0213 train_loss= 0.48112 val_roc= 0.91927 val_ap= 0.93586 time= 0.37447\n",
      "Epoch: 0214 train_loss= 0.48083 val_roc= 0.92114 val_ap= 0.93708 time= 0.36810\n",
      "Epoch: 0215 train_loss= 0.48463 val_roc= 0.92092 val_ap= 0.93700 time= 0.38406\n",
      "Epoch: 0216 train_loss= 0.48245 val_roc= 0.91947 val_ap= 0.93615 time= 0.41679\n",
      "Epoch: 0217 train_loss= 0.48360 val_roc= 0.91658 val_ap= 0.93425 time= 0.38509\n",
      "Epoch: 0218 train_loss= 0.48120 val_roc= 0.91421 val_ap= 0.93260 time= 0.37746\n",
      "Epoch: 0219 train_loss= 0.48435 val_roc= 0.91403 val_ap= 0.93248 time= 0.36685\n",
      "Epoch: 0220 train_loss= 0.48341 val_roc= 0.91614 val_ap= 0.93404 time= 0.38110\n",
      "Epoch: 0221 train_loss= 0.48158 val_roc= 0.91847 val_ap= 0.93561 time= 0.41974\n",
      "Epoch: 0222 train_loss= 0.48104 val_roc= 0.92059 val_ap= 0.93693 time= 0.40629\n",
      "Epoch: 0223 train_loss= 0.48039 val_roc= 0.92243 val_ap= 0.93806 time= 0.37667\n",
      "Epoch: 0224 train_loss= 0.48570 val_roc= 0.92209 val_ap= 0.93785 time= 0.37801\n",
      "Epoch: 0225 train_loss= 0.48606 val_roc= 0.91934 val_ap= 0.93610 time= 0.37887\n",
      "Epoch: 0226 train_loss= 0.48085 val_roc= 0.91687 val_ap= 0.93453 time= 0.41742\n",
      "Epoch: 0227 train_loss= 0.48023 val_roc= 0.91463 val_ap= 0.93303 time= 0.41314\n",
      "Epoch: 0228 train_loss= 0.48331 val_roc= 0.91466 val_ap= 0.93305 time= 0.37793\n",
      "Epoch: 0229 train_loss= 0.48334 val_roc= 0.91668 val_ap= 0.93445 time= 0.37599\n",
      "Epoch: 0230 train_loss= 0.48037 val_roc= 0.91907 val_ap= 0.93593 time= 0.37532\n",
      "Epoch: 0231 train_loss= 0.48010 val_roc= 0.92129 val_ap= 0.93729 time= 0.39039\n",
      "Epoch: 0232 train_loss= 0.48369 val_roc= 0.92134 val_ap= 0.93736 time= 0.41400\n",
      "Epoch: 0233 train_loss= 0.48091 val_roc= 0.92224 val_ap= 0.93794 time= 0.38266\n",
      "Epoch: 0234 train_loss= 0.48121 val_roc= 0.92190 val_ap= 0.93771 time= 0.37407\n",
      "Epoch: 0235 train_loss= 0.48129 val_roc= 0.92023 val_ap= 0.93666 time= 0.37076\n",
      "Epoch: 0236 train_loss= 0.47998 val_roc= 0.91800 val_ap= 0.93527 time= 0.38331\n",
      "Epoch: 0237 train_loss= 0.47874 val_roc= 0.91584 val_ap= 0.93382 time= 0.40947\n",
      "Epoch: 0238 train_loss= 0.48257 val_roc= 0.91632 val_ap= 0.93408 time= 0.39897\n",
      "Epoch: 0239 train_loss= 0.47966 val_roc= 0.91799 val_ap= 0.93511 time= 0.37674\n",
      "Epoch: 0240 train_loss= 0.47862 val_roc= 0.92024 val_ap= 0.93652 time= 0.37699\n",
      "Epoch: 0241 train_loss= 0.48053 val_roc= 0.92062 val_ap= 0.93679 time= 0.37559\n",
      "Epoch: 0242 train_loss= 0.47857 val_roc= 0.92060 val_ap= 0.93680 time= 0.41561\n",
      "Epoch: 0243 train_loss= 0.47902 val_roc= 0.91940 val_ap= 0.93608 time= 0.41726\n",
      "Epoch: 0244 train_loss= 0.47817 val_roc= 0.91951 val_ap= 0.93615 time= 0.37734\n",
      "Epoch: 0245 train_loss= 0.47732 val_roc= 0.91912 val_ap= 0.93597 time= 0.37577\n",
      "Epoch: 0246 train_loss= 0.47778 val_roc= 0.91981 val_ap= 0.93650 time= 0.37163\n",
      "Epoch: 0247 train_loss= 0.47745 val_roc= 0.92148 val_ap= 0.93756 time= 0.40537\n",
      "Epoch: 0248 train_loss= 0.47663 val_roc= 0.92263 val_ap= 0.93830 time= 0.40744\n",
      "Epoch: 0249 train_loss= 0.47728 val_roc= 0.92215 val_ap= 0.93806 time= 0.38296\n",
      "Epoch: 0250 train_loss= 0.47777 val_roc= 0.92019 val_ap= 0.93681 time= 0.37617\n",
      "Epoch: 0251 train_loss= 0.47732 val_roc= 0.91922 val_ap= 0.93623 time= 0.36818\n",
      "Epoch: 0252 train_loss= 0.47858 val_roc= 0.92052 val_ap= 0.93712 time= 0.38579\n",
      "Epoch: 0253 train_loss= 0.47958 val_roc= 0.92398 val_ap= 0.93936 time= 0.41744\n",
      "Epoch: 0254 train_loss= 0.47780 val_roc= 0.92485 val_ap= 0.93987 time= 0.39452\n",
      "Epoch: 0255 train_loss= 0.47492 val_roc= 0.92506 val_ap= 0.94005 time= 0.37938\n",
      "Epoch: 0256 train_loss= 0.47679 val_roc= 0.92379 val_ap= 0.93930 time= 0.38197\n",
      "Epoch: 0257 train_loss= 0.47340 val_roc= 0.92224 val_ap= 0.93821 time= 0.38567\n",
      "Epoch: 0258 train_loss= 0.47277 val_roc= 0.92086 val_ap= 0.93709 time= 0.41635\n",
      "Epoch: 0259 train_loss= 0.47341 val_roc= 0.91979 val_ap= 0.93623 time= 0.40524\n",
      "Epoch: 0260 train_loss= 0.47288 val_roc= 0.91949 val_ap= 0.93597 time= 0.37089\n",
      "Epoch: 0261 train_loss= 0.47586 val_roc= 0.92247 val_ap= 0.93806 time= 0.37762\n",
      "Epoch: 0262 train_loss= 0.47295 val_roc= 0.92378 val_ap= 0.93892 time= 0.37941\n",
      "Epoch: 0263 train_loss= 0.47362 val_roc= 0.92684 val_ap= 0.94094 time= 0.39943\n",
      "Epoch: 0264 train_loss= 0.47142 val_roc= 0.92911 val_ap= 0.94237 time= 0.41497\n",
      "Epoch: 0265 train_loss= 0.47129 val_roc= 0.93051 val_ap= 0.94322 time= 0.37906\n",
      "Epoch: 0266 train_loss= 0.47196 val_roc= 0.93069 val_ap= 0.94338 time= 0.37931\n",
      "Epoch: 0267 train_loss= 0.47156 val_roc= 0.92988 val_ap= 0.94282 time= 0.37447\n",
      "Epoch: 0268 train_loss= 0.47093 val_roc= 0.92858 val_ap= 0.94204 time= 0.38533\n",
      "Epoch: 0269 train_loss= 0.47033 val_roc= 0.92604 val_ap= 0.94044 time= 0.41380\n",
      "Epoch: 0270 train_loss= 0.46940 val_roc= 0.92224 val_ap= 0.93779 time= 0.38267\n",
      "Epoch: 0271 train_loss= 0.46929 val_roc= 0.91856 val_ap= 0.93501 time= 0.37751\n",
      "Epoch: 0272 train_loss= 0.47004 val_roc= 0.91761 val_ap= 0.93430 time= 0.37280\n",
      "Epoch: 0273 train_loss= 0.47252 val_roc= 0.92156 val_ap= 0.93716 time= 0.37981\n",
      "Epoch: 0274 train_loss= 0.47313 val_roc= 0.92918 val_ap= 0.94232 time= 0.41242\n",
      "Epoch: 0275 train_loss= 0.46844 val_roc= 0.93265 val_ap= 0.94458 time= 0.39792\n",
      "Epoch: 0276 train_loss= 0.47164 val_roc= 0.93307 val_ap= 0.94507 time= 0.37354\n",
      "Epoch: 0277 train_loss= 0.47548 val_roc= 0.92960 val_ap= 0.94296 time= 0.37456\n",
      "Epoch: 0278 train_loss= 0.46670 val_roc= 0.92369 val_ap= 0.93916 time= 0.37383\n",
      "Epoch: 0279 train_loss= 0.46705 val_roc= 0.91608 val_ap= 0.93419 time= 0.42170\n",
      "Epoch: 0280 train_loss= 0.47643 val_roc= 0.91820 val_ap= 0.93581 time= 0.40919\n",
      "Epoch: 0281 train_loss= 0.47336 val_roc= 0.92593 val_ap= 0.94107 time= 0.37396\n",
      "Epoch: 0282 train_loss= 0.46618 val_roc= 0.93122 val_ap= 0.94441 time= 0.37623\n",
      "Epoch: 0283 train_loss= 0.46937 val_roc= 0.93255 val_ap= 0.94509 time= 0.37913\n",
      "Epoch: 0284 train_loss= 0.46837 val_roc= 0.93090 val_ap= 0.94349 time= 0.41368\n",
      "Epoch: 0285 train_loss= 0.46641 val_roc= 0.92658 val_ap= 0.94056 time= 0.41305\n",
      "Epoch: 0286 train_loss= 0.46592 val_roc= 0.92128 val_ap= 0.93725 time= 0.37353\n",
      "Epoch: 0287 train_loss= 0.46965 val_roc= 0.92200 val_ap= 0.93789 time= 0.41120\n",
      "Epoch: 0288 train_loss= 0.46623 val_roc= 0.92399 val_ap= 0.93966 time= 0.38361\n",
      "Epoch: 0289 train_loss= 0.46506 val_roc= 0.92665 val_ap= 0.94163 time= 0.38655\n",
      "Epoch: 0290 train_loss= 0.46615 val_roc= 0.92967 val_ap= 0.94357 time= 0.44556\n",
      "Epoch: 0291 train_loss= 0.46435 val_roc= 0.93222 val_ap= 0.94504 time= 0.37880\n",
      "Epoch: 0292 train_loss= 0.46722 val_roc= 0.93100 val_ap= 0.94400 time= 0.37116\n",
      "Epoch: 0293 train_loss= 0.46427 val_roc= 0.92954 val_ap= 0.94278 time= 0.38074\n",
      "Epoch: 0294 train_loss= 0.46607 val_roc= 0.92517 val_ap= 0.93971 time= 0.37614\n",
      "Epoch: 0295 train_loss= 0.46473 val_roc= 0.92089 val_ap= 0.93692 time= 0.41533\n",
      "Epoch: 0296 train_loss= 0.46464 val_roc= 0.92061 val_ap= 0.93686 time= 0.39652\n",
      "Epoch: 0297 train_loss= 0.46365 val_roc= 0.92038 val_ap= 0.93688 time= 0.37304\n",
      "Epoch: 0298 train_loss= 0.46324 val_roc= 0.92096 val_ap= 0.93766 time= 0.37448\n",
      "Epoch: 0299 train_loss= 0.46388 val_roc= 0.92360 val_ap= 0.93964 time= 0.37214\n",
      "Epoch: 0300 train_loss= 0.46255 val_roc= 0.92557 val_ap= 0.94109 time= 0.41500\n",
      "Epoch: 0301 train_loss= 0.46368 val_roc= 0.92507 val_ap= 0.94080 time= 0.39993\n",
      "Epoch: 0302 train_loss= 0.46368 val_roc= 0.92570 val_ap= 0.94097 time= 0.38125\n",
      "Epoch: 0303 train_loss= 0.46318 val_roc= 0.92576 val_ap= 0.94094 time= 0.37880\n",
      "Epoch: 0304 train_loss= 0.46392 val_roc= 0.92846 val_ap= 0.94264 time= 0.37468\n",
      "Epoch: 0305 train_loss= 0.46275 val_roc= 0.93060 val_ap= 0.94414 time= 0.40037\n",
      "Epoch: 0306 train_loss= 0.46415 val_roc= 0.92985 val_ap= 0.94374 time= 0.42190\n",
      "Epoch: 0307 train_loss= 0.46377 val_roc= 0.92586 val_ap= 0.94143 time= 0.38057\n",
      "Epoch: 0308 train_loss= 0.46461 val_roc= 0.91734 val_ap= 0.93627 time= 0.36846\n",
      "Epoch: 0309 train_loss= 0.46915 val_roc= 0.91765 val_ap= 0.93634 time= 0.37837\n",
      "Epoch: 0310 train_loss= 0.46473 val_roc= 0.92292 val_ap= 0.93928 time= 0.38176\n",
      "Epoch: 0311 train_loss= 0.46268 val_roc= 0.92966 val_ap= 0.94313 time= 0.42143\n",
      "Epoch: 0312 train_loss= 0.46188 val_roc= 0.93301 val_ap= 0.94520 time= 0.39932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0313 train_loss= 0.46547 val_roc= 0.93138 val_ap= 0.94443 time= 0.37626\n",
      "Epoch: 0314 train_loss= 0.46173 val_roc= 0.92739 val_ap= 0.94237 time= 0.36649\n",
      "Epoch: 0315 train_loss= 0.46211 val_roc= 0.92305 val_ap= 0.93976 time= 0.38187\n",
      "Epoch: 0316 train_loss= 0.46209 val_roc= 0.92006 val_ap= 0.93821 time= 0.41935\n",
      "Epoch: 0317 train_loss= 0.46761 val_roc= 0.92429 val_ap= 0.94096 time= 0.41011\n",
      "Epoch: 0318 train_loss= 0.46219 val_roc= 0.93111 val_ap= 0.94508 time= 0.37662\n",
      "Epoch: 0319 train_loss= 0.46398 val_roc= 0.93332 val_ap= 0.94620 time= 0.36793\n",
      "Epoch: 0320 train_loss= 0.46360 val_roc= 0.93085 val_ap= 0.94450 time= 0.37663\n",
      "Epoch: 0321 train_loss= 0.46585 val_roc= 0.92217 val_ap= 0.93934 time= 0.40512\n",
      "Epoch: 0322 train_loss= 0.46033 val_roc= 0.91415 val_ap= 0.93415 time= 0.40867\n",
      "Epoch: 0323 train_loss= 0.46491 val_roc= 0.91357 val_ap= 0.93401 time= 0.37600\n",
      "Epoch: 0324 train_loss= 0.46758 val_roc= 0.92142 val_ap= 0.93935 time= 0.37866\n",
      "Epoch: 0325 train_loss= 0.46155 val_roc= 0.93223 val_ap= 0.94553 time= 0.37826\n",
      "Epoch: 0326 train_loss= 0.46193 val_roc= 0.93613 val_ap= 0.94810 time= 0.38741\n",
      "Epoch: 0327 train_loss= 0.46680 val_roc= 0.93476 val_ap= 0.94718 time= 0.40949\n",
      "Epoch: 0328 train_loss= 0.46319 val_roc= 0.92780 val_ap= 0.94315 time= 0.38315\n",
      "Epoch: 0329 train_loss= 0.45942 val_roc= 0.92029 val_ap= 0.93887 time= 0.37684\n",
      "Epoch: 0330 train_loss= 0.46019 val_roc= 0.91697 val_ap= 0.93677 time= 0.37968\n",
      "Epoch: 0331 train_loss= 0.47048 val_roc= 0.92285 val_ap= 0.94056 time= 0.40950\n",
      "Epoch: 0332 train_loss= 0.45888 val_roc= 0.92985 val_ap= 0.94447 time= 0.41283\n",
      "Epoch: 0333 train_loss= 0.45829 val_roc= 0.93472 val_ap= 0.94735 time= 0.39630\n",
      "Epoch: 0334 train_loss= 0.46167 val_roc= 0.93518 val_ap= 0.94781 time= 0.38440\n",
      "Epoch: 0335 train_loss= 0.45985 val_roc= 0.93422 val_ap= 0.94736 time= 0.37236\n",
      "Epoch: 0336 train_loss= 0.46039 val_roc= 0.92917 val_ap= 0.94454 time= 0.37766\n",
      "Epoch: 0337 train_loss= 0.45910 val_roc= 0.92689 val_ap= 0.94326 time= 0.40656\n",
      "Epoch: 0338 train_loss= 0.45806 val_roc= 0.92226 val_ap= 0.94049 time= 0.41668\n",
      "Epoch: 0339 train_loss= 0.46327 val_roc= 0.92485 val_ap= 0.94202 time= 0.37687\n",
      "Epoch: 0340 train_loss= 0.45725 val_roc= 0.92821 val_ap= 0.94396 time= 0.37692\n",
      "Epoch: 0341 train_loss= 0.45706 val_roc= 0.93047 val_ap= 0.94532 time= 0.37159\n",
      "Epoch: 0342 train_loss= 0.45790 val_roc= 0.93163 val_ap= 0.94620 time= 0.39696\n",
      "Epoch: 0343 train_loss= 0.45725 val_roc= 0.93125 val_ap= 0.94620 time= 0.41757\n",
      "Epoch: 0344 train_loss= 0.45758 val_roc= 0.93198 val_ap= 0.94658 time= 0.37514\n",
      "Epoch: 0345 train_loss= 0.45722 val_roc= 0.93034 val_ap= 0.94556 time= 0.38675\n",
      "Epoch: 0346 train_loss= 0.45642 val_roc= 0.92674 val_ap= 0.94342 time= 0.37934\n",
      "Epoch: 0347 train_loss= 0.45809 val_roc= 0.92621 val_ap= 0.94300 time= 0.38622\n",
      "Epoch: 0348 train_loss= 0.45637 val_roc= 0.92564 val_ap= 0.94280 time= 0.41300\n",
      "Epoch: 0349 train_loss= 0.46205 val_roc= 0.93080 val_ap= 0.94591 time= 0.38925\n",
      "Epoch: 0350 train_loss= 0.45510 val_roc= 0.93508 val_ap= 0.94830 time= 0.38327\n",
      "Epoch: 0351 train_loss= 0.45645 val_roc= 0.93867 val_ap= 0.95044 time= 0.37280\n",
      "Epoch: 0352 train_loss= 0.45655 val_roc= 0.93965 val_ap= 0.95121 time= 0.37886\n",
      "Epoch: 0353 train_loss= 0.46078 val_roc= 0.93582 val_ap= 0.94900 time= 0.41638\n",
      "Epoch: 0354 train_loss= 0.45800 val_roc= 0.93213 val_ap= 0.94702 time= 0.40141\n",
      "Epoch: 0355 train_loss= 0.45514 val_roc= 0.92772 val_ap= 0.94469 time= 0.37602\n",
      "Epoch: 0356 train_loss= 0.45521 val_roc= 0.92267 val_ap= 0.94168 time= 0.38286\n",
      "Epoch: 0357 train_loss= 0.45483 val_roc= 0.92002 val_ap= 0.93992 time= 0.40353\n",
      "Epoch: 0358 train_loss= 0.46285 val_roc= 0.92537 val_ap= 0.94335 time= 0.40815\n",
      "Epoch: 0359 train_loss= 0.45552 val_roc= 0.92924 val_ap= 0.94575 time= 0.40669\n",
      "Epoch: 0360 train_loss= 0.45550 val_roc= 0.93508 val_ap= 0.94913 time= 0.37305\n",
      "Epoch: 0361 train_loss= 0.45579 val_roc= 0.93779 val_ap= 0.95072 time= 0.38734\n",
      "Epoch: 0362 train_loss= 0.45523 val_roc= 0.93773 val_ap= 0.95074 time= 0.42148\n",
      "Epoch: 0363 train_loss= 0.45561 val_roc= 0.93780 val_ap= 0.95069 time= 0.40602\n",
      "Epoch: 0364 train_loss= 0.45554 val_roc= 0.93623 val_ap= 0.94959 time= 0.40918\n",
      "Epoch: 0365 train_loss= 0.45587 val_roc= 0.93585 val_ap= 0.94900 time= 0.37610\n",
      "Epoch: 0366 train_loss= 0.45434 val_roc= 0.93696 val_ap= 0.94942 time= 0.37942\n",
      "Epoch: 0367 train_loss= 0.45902 val_roc= 0.93483 val_ap= 0.94867 time= 0.37966\n",
      "Epoch: 0368 train_loss= 0.45620 val_roc= 0.93034 val_ap= 0.94636 time= 0.38575\n",
      "Epoch: 0369 train_loss= 0.45406 val_roc= 0.92576 val_ap= 0.94357 time= 0.40785\n",
      "Epoch: 0370 train_loss= 0.45623 val_roc= 0.92676 val_ap= 0.94416 time= 0.38940\n",
      "Epoch: 0371 train_loss= 0.45496 val_roc= 0.93210 val_ap= 0.94741 time= 0.37694\n",
      "Epoch: 0372 train_loss= 0.45368 val_roc= 0.93695 val_ap= 0.95017 time= 0.38257\n",
      "Epoch: 0373 train_loss= 0.45351 val_roc= 0.93965 val_ap= 0.95165 time= 0.38093\n",
      "Epoch: 0374 train_loss= 0.45314 val_roc= 0.94197 val_ap= 0.95281 time= 0.41488\n",
      "Epoch: 0375 train_loss= 0.45301 val_roc= 0.94287 val_ap= 0.95319 time= 0.41111\n",
      "Epoch: 0376 train_loss= 0.46292 val_roc= 0.93880 val_ap= 0.95097 time= 0.37978\n",
      "Epoch: 0377 train_loss= 0.45864 val_roc= 0.92766 val_ap= 0.94470 time= 0.38550\n",
      "Epoch: 0378 train_loss= 0.46409 val_roc= 0.92533 val_ap= 0.94318 time= 0.37262\n",
      "Epoch: 0379 train_loss= 0.45282 val_roc= 0.92544 val_ap= 0.94312 time= 0.40895\n",
      "Epoch: 0380 train_loss= 0.45738 val_roc= 0.93291 val_ap= 0.94759 time= 0.41678\n",
      "Epoch: 0381 train_loss= 0.45720 val_roc= 0.94160 val_ap= 0.95265 time= 0.38218\n",
      "Epoch: 0382 train_loss= 0.45448 val_roc= 0.94426 val_ap= 0.95412 time= 0.38759\n",
      "Epoch: 0383 train_loss= 0.46313 val_roc= 0.94293 val_ap= 0.95344 time= 0.37670\n",
      "Epoch: 0384 train_loss= 0.45517 val_roc= 0.93894 val_ap= 0.95102 time= 0.39856\n",
      "Epoch: 0385 train_loss= 0.45237 val_roc= 0.93052 val_ap= 0.94607 time= 0.41340\n",
      "Epoch: 0386 train_loss= 0.45169 val_roc= 0.92207 val_ap= 0.94061 time= 0.36656\n",
      "Epoch: 0387 train_loss= 0.45925 val_roc= 0.92360 val_ap= 0.94134 time= 0.37125\n",
      "Epoch: 0388 train_loss= 0.45887 val_roc= 0.93305 val_ap= 0.94719 time= 0.37098\n",
      "Epoch: 0389 train_loss= 0.45221 val_roc= 0.93932 val_ap= 0.95106 time= 0.38007\n",
      "Epoch: 0390 train_loss= 0.45559 val_roc= 0.93947 val_ap= 0.95135 time= 0.41120\n",
      "Epoch: 0391 train_loss= 0.45182 val_roc= 0.93827 val_ap= 0.95079 time= 0.39323\n",
      "Epoch: 0392 train_loss= 0.45419 val_roc= 0.93422 val_ap= 0.94850 time= 0.36759\n",
      "Epoch: 0393 train_loss= 0.45098 val_roc= 0.92947 val_ap= 0.94550 time= 0.37777\n",
      "Epoch: 0394 train_loss= 0.45409 val_roc= 0.92847 val_ap= 0.94460 time= 0.38221\n",
      "Epoch: 0395 train_loss= 0.45396 val_roc= 0.93255 val_ap= 0.94700 time= 0.41726\n",
      "Epoch: 0396 train_loss= 0.45363 val_roc= 0.93894 val_ap= 0.95054 time= 0.40336\n",
      "Epoch: 0397 train_loss= 0.45369 val_roc= 0.94092 val_ap= 0.95161 time= 0.36823\n",
      "Epoch: 0398 train_loss= 0.45238 val_roc= 0.94282 val_ap= 0.95293 time= 0.37326\n",
      "Epoch: 0399 train_loss= 0.45124 val_roc= 0.94296 val_ap= 0.95317 time= 0.36989\n",
      "Epoch: 0400 train_loss= 0.45363 val_roc= 0.94054 val_ap= 0.95194 time= 0.40306\n",
      "Epoch: 0401 train_loss= 0.45125 val_roc= 0.93631 val_ap= 0.94967 time= 0.41156\n",
      "Epoch: 0402 train_loss= 0.45044 val_roc= 0.93094 val_ap= 0.94646 time= 0.38138\n",
      "Epoch: 0403 train_loss= 0.45083 val_roc= 0.92530 val_ap= 0.94275 time= 0.37279\n",
      "Epoch: 0404 train_loss= 0.45320 val_roc= 0.92559 val_ap= 0.94289 time= 0.37385\n",
      "Epoch: 0405 train_loss= 0.45523 val_roc= 0.93337 val_ap= 0.94781 time= 0.38584\n",
      "Epoch: 0406 train_loss= 0.45049 val_roc= 0.93801 val_ap= 0.95061 time= 0.41224\n",
      "Epoch: 0407 train_loss= 0.45047 val_roc= 0.94055 val_ap= 0.95214 time= 0.39667\n",
      "Epoch: 0408 train_loss= 0.45149 val_roc= 0.94390 val_ap= 0.95410 time= 0.37927\n",
      "Epoch: 0409 train_loss= 0.46274 val_roc= 0.93922 val_ap= 0.95134 time= 0.37595\n",
      "Epoch: 0410 train_loss= 0.44871 val_roc= 0.93357 val_ap= 0.94785 time= 0.38090\n",
      "Epoch: 0411 train_loss= 0.44981 val_roc= 0.92917 val_ap= 0.94496 time= 0.41645\n",
      "Epoch: 0412 train_loss= 0.45317 val_roc= 0.93002 val_ap= 0.94555 time= 0.40256\n",
      "Epoch: 0413 train_loss= 0.45464 val_roc= 0.93631 val_ap= 0.94974 time= 0.37787\n",
      "Epoch: 0414 train_loss= 0.44977 val_roc= 0.94107 val_ap= 0.95245 time= 0.37372\n",
      "Epoch: 0415 train_loss= 0.45322 val_roc= 0.94256 val_ap= 0.95327 time= 0.37658\n",
      "Epoch: 0416 train_loss= 0.45314 val_roc= 0.94111 val_ap= 0.95242 time= 0.41753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0417 train_loss= 0.44904 val_roc= 0.93768 val_ap= 0.95024 time= 0.40966\n",
      "Epoch: 0418 train_loss= 0.44977 val_roc= 0.93564 val_ap= 0.94877 time= 0.38672\n",
      "Epoch: 0419 train_loss= 0.44982 val_roc= 0.93632 val_ap= 0.94920 time= 0.37513\n",
      "Epoch: 0420 train_loss= 0.45326 val_roc= 0.94124 val_ap= 0.95239 time= 0.37633\n",
      "Epoch: 0421 train_loss= 0.45235 val_roc= 0.94650 val_ap= 0.95565 time= 0.40428\n",
      "Epoch: 0422 train_loss= 0.45591 val_roc= 0.94705 val_ap= 0.95585 time= 0.41653\n",
      "Epoch: 0423 train_loss= 0.47563 val_roc= 0.93950 val_ap= 0.95130 time= 0.37874\n",
      "Epoch: 0424 train_loss= 0.44967 val_roc= 0.92284 val_ap= 0.94075 time= 0.37096\n",
      "Epoch: 0425 train_loss= 0.45926 val_roc= 0.91491 val_ap= 0.93566 time= 0.37989\n",
      "Epoch: 0426 train_loss= 0.45812 val_roc= 0.92106 val_ap= 0.93969 time= 0.38484\n",
      "Epoch: 0427 train_loss= 0.46286 val_roc= 0.93452 val_ap= 0.94798 time= 0.40935\n",
      "Epoch: 0428 train_loss= 0.45024 val_roc= 0.94173 val_ap= 0.95221 time= 0.39139\n",
      "Epoch: 0429 train_loss= 0.45299 val_roc= 0.94517 val_ap= 0.95406 time= 0.38354\n",
      "Epoch: 0430 train_loss= 0.45808 val_roc= 0.94463 val_ap= 0.95375 time= 0.38439\n",
      "Epoch: 0431 train_loss= 0.45606 val_roc= 0.94156 val_ap= 0.95234 time= 0.38971\n",
      "Epoch: 0432 train_loss= 0.45299 val_roc= 0.93408 val_ap= 0.94789 time= 0.41075\n",
      "Epoch: 0433 train_loss= 0.45190 val_roc= 0.92875 val_ap= 0.94450 time= 0.40568\n",
      "Epoch: 0434 train_loss= 0.45680 val_roc= 0.93133 val_ap= 0.94643 time= 0.37735\n",
      "Epoch: 0435 train_loss= 0.45979 val_roc= 0.94005 val_ap= 0.95245 time= 0.38506\n",
      "Epoch: 0436 train_loss= 0.44916 val_roc= 0.94482 val_ap= 0.95516 time= 0.37749\n",
      "Epoch: 0437 train_loss= 0.45172 val_roc= 0.94662 val_ap= 0.95595 time= 0.41221\n",
      "Epoch: 0438 train_loss= 0.45267 val_roc= 0.94521 val_ap= 0.95470 time= 0.41446\n",
      "Epoch: 0439 train_loss= 0.45018 val_roc= 0.94202 val_ap= 0.95246 time= 0.40280\n",
      "Epoch: 0440 train_loss= 0.45011 val_roc= 0.93733 val_ap= 0.94941 time= 0.38080\n",
      "Epoch: 0441 train_loss= 0.44978 val_roc= 0.93514 val_ap= 0.94827 time= 0.37471\n",
      "Epoch: 0442 train_loss= 0.44875 val_roc= 0.93307 val_ap= 0.94713 time= 0.39625\n",
      "Epoch: 0443 train_loss= 0.44844 val_roc= 0.93291 val_ap= 0.94721 time= 0.42432\n",
      "Epoch: 0444 train_loss= 0.44956 val_roc= 0.93458 val_ap= 0.94867 time= 0.37778\n",
      "Epoch: 0445 train_loss= 0.44822 val_roc= 0.93730 val_ap= 0.95062 time= 0.37226\n",
      "Epoch: 0446 train_loss= 0.44804 val_roc= 0.93811 val_ap= 0.95124 time= 0.38709\n",
      "Epoch: 0447 train_loss= 0.44772 val_roc= 0.93808 val_ap= 0.95134 time= 0.37974\n",
      "Epoch: 0448 train_loss= 0.44699 val_roc= 0.93775 val_ap= 0.95119 time= 0.41866\n",
      "Epoch: 0449 train_loss= 0.44830 val_roc= 0.93748 val_ap= 0.95088 time= 0.49778\n",
      "Epoch: 0450 train_loss= 0.44844 val_roc= 0.93542 val_ap= 0.94932 time= 0.46336\n",
      "Epoch: 0451 train_loss= 0.45153 val_roc= 0.93787 val_ap= 0.95077 time= 0.40237\n",
      "Epoch: 0452 train_loss= 0.44660 val_roc= 0.93933 val_ap= 0.95139 time= 0.46824\n",
      "Epoch: 0453 train_loss= 0.44890 val_roc= 0.93885 val_ap= 0.95111 time= 0.47649\n",
      "Epoch: 0454 train_loss= 0.44875 val_roc= 0.93627 val_ap= 0.94954 time= 0.45164\n",
      "Epoch: 0455 train_loss= 0.44904 val_roc= 0.93518 val_ap= 0.94895 time= 0.44759\n",
      "Epoch: 0456 train_loss= 0.44750 val_roc= 0.93598 val_ap= 0.94951 time= 0.49034\n",
      "Epoch: 0457 train_loss= 0.44782 val_roc= 0.93467 val_ap= 0.94868 time= 0.51084\n",
      "Epoch: 0458 train_loss= 0.44789 val_roc= 0.93358 val_ap= 0.94800 time= 0.46327\n",
      "Epoch: 0459 train_loss= 0.44843 val_roc= 0.93250 val_ap= 0.94738 time= 0.39469\n",
      "Epoch: 0460 train_loss= 0.44695 val_roc= 0.93239 val_ap= 0.94724 time= 0.47145\n",
      "Epoch: 0461 train_loss= 0.44778 val_roc= 0.93478 val_ap= 0.94872 time= 0.45279\n",
      "Epoch: 0462 train_loss= 0.45339 val_roc= 0.94041 val_ap= 0.95211 time= 0.41901\n",
      "Epoch: 0463 train_loss= 0.44653 val_roc= 0.94386 val_ap= 0.95385 time= 0.39842\n",
      "Epoch: 0464 train_loss= 0.45228 val_roc= 0.94384 val_ap= 0.95376 time= 0.46642\n",
      "Epoch: 0465 train_loss= 0.45538 val_roc= 0.93888 val_ap= 0.95093 time= 0.46422\n",
      "Epoch: 0466 train_loss= 0.44900 val_roc= 0.92885 val_ap= 0.94457 time= 0.48140\n",
      "Epoch: 0467 train_loss= 0.45242 val_roc= 0.92379 val_ap= 0.94156 time= 0.42401\n",
      "Epoch: 0468 train_loss= 0.44754 val_roc= 0.92405 val_ap= 0.94167 time= 0.47392\n",
      "Epoch: 0469 train_loss= 0.44730 val_roc= 0.92704 val_ap= 0.94347 time= 0.40596\n",
      "Epoch: 0470 train_loss= 0.44576 val_roc= 0.92998 val_ap= 0.94527 time= 0.39149\n",
      "Epoch: 0471 train_loss= 0.45032 val_roc= 0.93538 val_ap= 0.94880 time= 0.43166\n",
      "Epoch: 0472 train_loss= 0.44635 val_roc= 0.93965 val_ap= 0.95142 time= 0.40433\n",
      "Epoch: 0473 train_loss= 0.44697 val_roc= 0.94310 val_ap= 0.95331 time= 0.40791\n",
      "Epoch: 0474 train_loss= 0.45168 val_roc= 0.94266 val_ap= 0.95303 time= 0.39634\n",
      "Epoch: 0475 train_loss= 0.44873 val_roc= 0.93961 val_ap= 0.95126 time= 0.40076\n",
      "Epoch: 0476 train_loss= 0.44623 val_roc= 0.93692 val_ap= 0.94949 time= 0.41754\n",
      "Epoch: 0477 train_loss= 0.44674 val_roc= 0.93260 val_ap= 0.94659 time= 0.41321\n",
      "Epoch: 0478 train_loss= 0.44915 val_roc= 0.93180 val_ap= 0.94622 time= 0.41238\n",
      "Epoch: 0479 train_loss= 0.44822 val_roc= 0.93544 val_ap= 0.94878 time= 0.40046\n",
      "Epoch: 0480 train_loss= 0.44584 val_roc= 0.93832 val_ap= 0.95074 time= 0.38812\n",
      "Epoch: 0481 train_loss= 0.44505 val_roc= 0.93912 val_ap= 0.95117 time= 0.43341\n",
      "Epoch: 0482 train_loss= 0.44731 val_roc= 0.93834 val_ap= 0.95069 time= 0.43079\n",
      "Epoch: 0483 train_loss= 0.44867 val_roc= 0.93912 val_ap= 0.95114 time= 0.40539\n",
      "Epoch: 0484 train_loss= 0.44529 val_roc= 0.93950 val_ap= 0.95144 time= 0.39760\n",
      "Epoch: 0485 train_loss= 0.45166 val_roc= 0.93685 val_ap= 0.94973 time= 0.41803\n",
      "Epoch: 0486 train_loss= 0.44599 val_roc= 0.93316 val_ap= 0.94738 time= 0.42560\n",
      "Epoch: 0487 train_loss= 0.44513 val_roc= 0.92753 val_ap= 0.94372 time= 0.40009\n",
      "Epoch: 0488 train_loss= 0.44902 val_roc= 0.92843 val_ap= 0.94437 time= 0.48619\n",
      "Epoch: 0489 train_loss= 0.44651 val_roc= 0.93326 val_ap= 0.94760 time= 0.44704\n",
      "Epoch: 0490 train_loss= 0.44479 val_roc= 0.93551 val_ap= 0.94911 time= 0.48113\n",
      "Epoch: 0491 train_loss= 0.45457 val_roc= 0.94236 val_ap= 0.95318 time= 0.46177\n",
      "Epoch: 0492 train_loss= 0.44714 val_roc= 0.94498 val_ap= 0.95468 time= 0.45459\n",
      "Epoch: 0493 train_loss= 0.45316 val_roc= 0.94282 val_ap= 0.95332 time= 0.47407\n",
      "Epoch: 0494 train_loss= 0.44917 val_roc= 0.93692 val_ap= 0.94976 time= 0.42312\n",
      "Epoch: 0495 train_loss= 0.45318 val_roc= 0.93651 val_ap= 0.94948 time= 0.48388\n",
      "Epoch: 0496 train_loss= 0.44534 val_roc= 0.93799 val_ap= 0.95032 time= 0.45289\n",
      "Epoch: 0497 train_loss= 0.44971 val_roc= 0.94239 val_ap= 0.95292 time= 0.43169\n",
      "Epoch: 0498 train_loss= 0.44892 val_roc= 0.94328 val_ap= 0.95348 time= 0.37211\n",
      "Epoch: 0499 train_loss= 0.44881 val_roc= 0.94121 val_ap= 0.95245 time= 0.42105\n",
      "Epoch: 0500 train_loss= 0.44670 val_roc= 0.93573 val_ap= 0.94925 time= 0.45354\n",
      "Epoch: 0501 train_loss= 0.44878 val_roc= 0.93427 val_ap= 0.94835 time= 0.38307\n",
      "Epoch: 0502 train_loss= 0.44453 val_roc= 0.93179 val_ap= 0.94675 time= 0.37189\n",
      "Epoch: 0503 train_loss= 0.44454 val_roc= 0.92848 val_ap= 0.94461 time= 0.42406\n",
      "Epoch: 0504 train_loss= 0.45309 val_roc= 0.93391 val_ap= 0.94807 time= 0.45491\n",
      "Epoch: 0505 train_loss= 0.44460 val_roc= 0.93896 val_ap= 0.95103 time= 0.41435\n",
      "Epoch: 0506 train_loss= 0.44407 val_roc= 0.94217 val_ap= 0.95270 time= 0.38103\n",
      "Epoch: 0507 train_loss= 0.44643 val_roc= 0.94216 val_ap= 0.95257 time= 0.37280\n",
      "Epoch: 0508 train_loss= 0.44997 val_roc= 0.93781 val_ap= 0.94979 time= 0.45204\n",
      "Epoch: 0509 train_loss= 0.44369 val_roc= 0.93196 val_ap= 0.94623 time= 0.45944\n",
      "Epoch: 0510 train_loss= 0.44389 val_roc= 0.92338 val_ap= 0.94079 time= 0.47067\n",
      "Epoch: 0511 train_loss= 0.45339 val_roc= 0.92644 val_ap= 0.94284 time= 0.46427\n",
      "Epoch: 0512 train_loss= 0.44501 val_roc= 0.93238 val_ap= 0.94687 time= 0.46129\n",
      "Epoch: 0513 train_loss= 0.45031 val_roc= 0.94155 val_ap= 0.95269 time= 0.37979\n",
      "Epoch: 0514 train_loss= 0.44503 val_roc= 0.94555 val_ap= 0.95510 time= 0.47363\n",
      "Epoch: 0515 train_loss= 0.44927 val_roc= 0.94590 val_ap= 0.95539 time= 0.44400\n",
      "Epoch: 0516 train_loss= 0.45028 val_roc= 0.94157 val_ap= 0.95273 time= 0.45638\n",
      "Epoch: 0517 train_loss= 0.44563 val_roc= 0.93317 val_ap= 0.94724 time= 0.37406\n",
      "Epoch: 0518 train_loss= 0.44342 val_roc= 0.92392 val_ap= 0.94105 time= 0.45054\n",
      "Epoch: 0519 train_loss= 0.45063 val_roc= 0.92425 val_ap= 0.94116 time= 0.48221\n",
      "Epoch: 0520 train_loss= 0.44595 val_roc= 0.92982 val_ap= 0.94479 time= 0.43881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0521 train_loss= 0.44507 val_roc= 0.93264 val_ap= 0.94688 time= 0.38698\n",
      "Epoch: 0522 train_loss= 0.44262 val_roc= 0.93553 val_ap= 0.94895 time= 0.48271\n",
      "Epoch: 0523 train_loss= 0.44516 val_roc= 0.94003 val_ap= 0.95207 time= 0.43410\n",
      "Epoch: 0524 train_loss= 0.44480 val_roc= 0.94079 val_ap= 0.95274 time= 0.41160\n",
      "Epoch: 0525 train_loss= 0.44399 val_roc= 0.94081 val_ap= 0.95277 time= 0.38432\n",
      "Epoch: 0526 train_loss= 0.44893 val_roc= 0.93745 val_ap= 0.95069 time= 0.38740\n",
      "Epoch: 0527 train_loss= 0.44361 val_roc= 0.93202 val_ap= 0.94685 time= 0.37547\n",
      "Epoch: 0528 train_loss= 0.44248 val_roc= 0.92620 val_ap= 0.94285 time= 0.39977\n",
      "Epoch: 0529 train_loss= 0.45106 val_roc= 0.93098 val_ap= 0.94587 time= 0.49091\n",
      "Epoch: 0530 train_loss= 0.44382 val_roc= 0.93726 val_ap= 0.94974 time= 0.39513\n",
      "Epoch: 0531 train_loss= 0.44377 val_roc= 0.94269 val_ap= 0.95326 time= 0.38142\n",
      "Epoch: 0532 train_loss= 0.44348 val_roc= 0.94623 val_ap= 0.95541 time= 0.37470\n",
      "Epoch: 0533 train_loss= 0.44607 val_roc= 0.94663 val_ap= 0.95583 time= 0.37680\n",
      "Epoch: 0534 train_loss= 0.45124 val_roc= 0.94304 val_ap= 0.95395 time= 0.41340\n",
      "Epoch: 0535 train_loss= 0.44267 val_roc= 0.93877 val_ap= 0.95146 time= 0.40183\n",
      "Epoch: 0536 train_loss= 0.44427 val_roc= 0.93175 val_ap= 0.94679 time= 0.38647\n",
      "Epoch: 0537 train_loss= 0.44747 val_roc= 0.92920 val_ap= 0.94498 time= 0.47185\n",
      "Epoch: 0538 train_loss= 0.44335 val_roc= 0.92688 val_ap= 0.94344 time= 0.48465\n",
      "Epoch: 0539 train_loss= 0.44377 val_roc= 0.92967 val_ap= 0.94542 time= 0.45505\n",
      "Epoch: 0540 train_loss= 0.44729 val_roc= 0.93848 val_ap= 0.95087 time= 0.37783\n",
      "Epoch: 0541 train_loss= 0.44342 val_roc= 0.94219 val_ap= 0.95298 time= 0.47487\n",
      "Epoch: 0542 train_loss= 0.44533 val_roc= 0.94198 val_ap= 0.95293 time= 0.40014\n",
      "Epoch: 0543 train_loss= 0.44381 val_roc= 0.94064 val_ap= 0.95216 time= 0.41269\n",
      "Epoch: 0544 train_loss= 0.44443 val_roc= 0.94143 val_ap= 0.95280 time= 0.40828\n",
      "Epoch: 0545 train_loss= 0.44238 val_roc= 0.94167 val_ap= 0.95286 time= 0.37498\n",
      "Epoch: 0546 train_loss= 0.44483 val_roc= 0.93991 val_ap= 0.95175 time= 0.37406\n",
      "Epoch: 0547 train_loss= 0.44660 val_roc= 0.94167 val_ap= 0.95262 time= 0.37167\n",
      "Epoch: 0548 train_loss= 0.44380 val_roc= 0.94174 val_ap= 0.95271 time= 0.38511\n",
      "Epoch: 0549 train_loss= 0.44195 val_roc= 0.94074 val_ap= 0.95203 time= 0.40944\n",
      "Epoch: 0550 train_loss= 0.44312 val_roc= 0.93641 val_ap= 0.94925 time= 0.40788\n",
      "Epoch: 0551 train_loss= 0.44309 val_roc= 0.93290 val_ap= 0.94700 time= 0.37105\n",
      "Epoch: 0552 train_loss= 0.44281 val_roc= 0.93235 val_ap= 0.94673 time= 0.37775\n",
      "Epoch: 0553 train_loss= 0.44892 val_roc= 0.93853 val_ap= 0.95076 time= 0.38236\n",
      "Epoch: 0554 train_loss= 0.44222 val_roc= 0.94474 val_ap= 0.95454 time= 0.40589\n",
      "Epoch: 0555 train_loss= 0.44365 val_roc= 0.94685 val_ap= 0.95594 time= 0.40439\n",
      "Epoch: 0556 train_loss= 0.44446 val_roc= 0.94504 val_ap= 0.95495 time= 0.40775\n",
      "Epoch: 0557 train_loss= 0.44385 val_roc= 0.93969 val_ap= 0.95178 time= 0.47936\n",
      "Epoch: 0558 train_loss= 0.44590 val_roc= 0.92926 val_ap= 0.94524 time= 0.38204\n",
      "Epoch: 0559 train_loss= 0.44020 val_roc= 0.91729 val_ap= 0.93722 time= 0.41291\n",
      "Epoch: 0560 train_loss= 0.45912 val_roc= 0.92092 val_ap= 0.93978 time= 0.40291\n",
      "Epoch: 0561 train_loss= 0.45766 val_roc= 0.93605 val_ap= 0.94909 time= 0.37030\n",
      "Epoch: 0562 train_loss= 0.44324 val_roc= 0.94532 val_ap= 0.95466 time= 0.37010\n",
      "Epoch: 0563 train_loss= 0.44580 val_roc= 0.94830 val_ap= 0.95642 time= 0.36858\n",
      "Epoch: 0564 train_loss= 0.44515 val_roc= 0.94863 val_ap= 0.95682 time= 0.39381\n",
      "Epoch: 0565 train_loss= 0.45597 val_roc= 0.94462 val_ap= 0.95475 time= 0.41304\n",
      "Epoch: 0566 train_loss= 0.44838 val_roc= 0.93508 val_ap= 0.94875 time= 0.38538\n",
      "Epoch: 0567 train_loss= 0.44045 val_roc= 0.91648 val_ap= 0.93622 time= 0.37546\n",
      "Epoch: 0568 train_loss= 0.44823 val_roc= 0.90557 val_ap= 0.92898 time= 0.37593\n",
      "Epoch: 0569 train_loss= 0.46587 val_roc= 0.92030 val_ap= 0.93941 time= 0.38015\n",
      "Epoch: 0570 train_loss= 0.44895 val_roc= 0.93745 val_ap= 0.95078 time= 0.40891\n",
      "Epoch: 0571 train_loss= 0.45272 val_roc= 0.94914 val_ap= 0.95781 time= 0.38070\n",
      "Epoch: 0572 train_loss= 0.44149 val_roc= 0.95239 val_ap= 0.95950 time= 0.37599\n",
      "Epoch: 0573 train_loss= 0.47223 val_roc= 0.95257 val_ap= 0.95935 time= 0.37437\n",
      "Epoch: 0574 train_loss= 0.46929 val_roc= 0.95006 val_ap= 0.95783 time= 0.38488\n",
      "Epoch: 0575 train_loss= 0.45652 val_roc= 0.94145 val_ap= 0.95270 time= 0.42286\n",
      "Epoch: 0576 train_loss= 0.44828 val_roc= 0.92588 val_ap= 0.94265 time= 0.40153\n",
      "Epoch: 0577 train_loss= 0.45103 val_roc= 0.91013 val_ap= 0.93145 time= 0.36984\n",
      "Epoch: 0578 train_loss= 0.47151 val_roc= 0.91223 val_ap= 0.93273 time= 0.36659\n",
      "Epoch: 0579 train_loss= 0.45557 val_roc= 0.92470 val_ap= 0.94167 time= 0.36977\n",
      "Epoch: 0580 train_loss= 0.44501 val_roc= 0.93490 val_ap= 0.94879 time= 0.40444\n",
      "Epoch: 0581 train_loss= 0.44290 val_roc= 0.94305 val_ap= 0.95399 time= 0.40614\n",
      "Epoch: 0582 train_loss= 0.44759 val_roc= 0.94893 val_ap= 0.95746 time= 0.38000\n",
      "Epoch: 0583 train_loss= 0.45710 val_roc= 0.94978 val_ap= 0.95784 time= 0.36609\n",
      "Epoch: 0584 train_loss= 0.45626 val_roc= 0.94815 val_ap= 0.95696 time= 0.37563\n",
      "Epoch: 0585 train_loss= 0.45718 val_roc= 0.94241 val_ap= 0.95349 time= 0.39213\n",
      "Epoch: 0586 train_loss= 0.44253 val_roc= 0.93198 val_ap= 0.94725 time= 0.40656\n",
      "Epoch: 0587 train_loss= 0.44829 val_roc= 0.92475 val_ap= 0.94230 time= 0.37981\n",
      "Epoch: 0588 train_loss= 0.45007 val_roc= 0.92436 val_ap= 0.94193 time= 0.36658\n",
      "Epoch: 0589 train_loss= 0.44867 val_roc= 0.92894 val_ap= 0.94504 time= 0.37218\n",
      "Epoch: 0590 train_loss= 0.44420 val_roc= 0.93512 val_ap= 0.94923 time= 0.37531\n",
      "Epoch: 0591 train_loss= 0.44254 val_roc= 0.93978 val_ap= 0.95206 time= 0.41734\n",
      "Epoch: 0592 train_loss= 0.44359 val_roc= 0.94381 val_ap= 0.95457 time= 0.40911\n",
      "Epoch: 0593 train_loss= 0.44687 val_roc= 0.94531 val_ap= 0.95568 time= 0.36932\n",
      "Epoch: 0594 train_loss= 0.44565 val_roc= 0.94441 val_ap= 0.95512 time= 0.37547\n",
      "Epoch: 0595 train_loss= 0.44279 val_roc= 0.94039 val_ap= 0.95257 time= 0.38545\n",
      "Epoch: 0596 train_loss= 0.44036 val_roc= 0.93432 val_ap= 0.94852 time= 0.40184\n",
      "Epoch: 0597 train_loss= 0.44719 val_roc= 0.93245 val_ap= 0.94724 time= 0.39973\n",
      "Epoch: 0598 train_loss= 0.44476 val_roc= 0.93439 val_ap= 0.94852 time= 0.38015\n",
      "Epoch: 0599 train_loss= 0.44313 val_roc= 0.93814 val_ap= 0.95090 time= 0.37044\n",
      "Epoch: 0600 train_loss= 0.44280 val_roc= 0.93934 val_ap= 0.95141 time= 0.37514\n",
      "Optimization Finished!\n",
      "Test ROC score: 0.94741\n",
      "Test AP score: 0.95797\n"
     ]
    }
   ],
   "source": [
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "adj_label = adj_train + sp.eye(adj_train.shape[0])\n",
    "adj_label = sparse_to_tuple(adj_label)\n",
    "\n",
    "# Train model\n",
    "for epoch in range(FLAGS.epochs):\n",
    "    t = time.time()\n",
    "    # Construct feed dictionary\n",
    "    feed_dict = construct_feed_dict(adj_norm, adj_label, features, placeholders)\n",
    "    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "    # One update of parameter matrices\n",
    "    _, avg_cost = sess.run([opt.opt_op, opt.cost], feed_dict=feed_dict)\n",
    "    # Performance on validation set\n",
    "    roc_curr, ap_curr = get_roc_score(val_edges, val_edges_false)\n",
    "\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \n",
    "          \"train_loss=\", \"{:.5f}\".format(avg_cost),\n",
    "          \"val_roc=\", \"{:.5f}\".format(roc_curr),\n",
    "          \"val_ap=\", \"{:.5f}\".format(ap_curr),\n",
    "          \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "print('Optimization Finished!')\n",
    "\n",
    "roc_score, ap_score = get_roc_score(test_edges, test_edges_false)\n",
    "print('Test ROC score: {:.5f}'.format(roc_score))\n",
    "print('Test AP score: {:.5f}'.format(ap_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
